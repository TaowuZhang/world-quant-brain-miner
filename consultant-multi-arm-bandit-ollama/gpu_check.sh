#!/bin/bash

echo "üîç GPU Access Diagnostic Script"
echo "================================"

# Check if we're in a container
echo "üì¶ Container Environment:"
if [ -f /.dockerenv ]; then
    echo "‚úÖ Running inside Docker container"
else
    echo "‚ö†Ô∏è  Not running inside Docker container"
fi

# Check NVIDIA runtime
echo -e "\nüñ•Ô∏è  NVIDIA Runtime Check:"
if command -v nvidia-smi &> /dev/null; then
    echo "‚úÖ nvidia-smi available"
    nvidia-smi --version | head -1
else
    echo "‚ùå nvidia-smi not available"
fi

# Check GPU devices
echo -e "\nüéØ GPU Device Check:"
if nvidia-smi &> /dev/null; then
    echo "‚úÖ GPU devices detected:"
    nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader
else
    echo "‚ùå No GPU devices detected"
    echo "   This could mean:"
    echo "   - NVIDIA Docker runtime not configured"
    echo "   - GPU not passed to container"
    echo "   - NVIDIA drivers not installed"
fi

# Check CUDA
echo -e "\nüîß CUDA Check:"
if command -v nvcc &> /dev/null; then
    echo "‚úÖ CUDA compiler available"
    nvcc --version | head -1
else
    echo "‚ö†Ô∏è  CUDA compiler not available"
fi

# Check environment variables
echo -e "\nüåç GPU Environment Variables:"
env | grep -E "(CUDA|NVIDIA|GPU)" | sort

# Check Ollama GPU support
echo -e "\nü§ñ Ollama GPU Support:"
if command -v ollama &> /dev/null; then
    echo "‚úÖ Ollama available"
    ollama --version
    
    # Check if Ollama can see GPU
    echo -e "\nüîç Ollama GPU Detection:"
    if ollama list &> /dev/null; then
        echo "‚úÖ Ollama can list models"
    else
        echo "‚ùå Ollama cannot list models"
    fi
else
    echo "‚ùå Ollama not available"
fi

# Check GPU libraries
echo -e "\nüìö GPU Libraries:"
if [ -f /usr/lib/x86_64-linux-gnu/libcuda.so* ]; then
    echo "‚úÖ CUDA libraries found"
    ls -la /usr/lib/x86_64-linux-gnu/libcuda.so* | head -3
else
    echo "‚ùå CUDA libraries not found"
fi

# Check if running with --gpus flag
echo -e "\nüöÄ Docker GPU Access:"
if [ -n "$NVIDIA_VISIBLE_DEVICES" ]; then
    echo "‚úÖ NVIDIA_VISIBLE_DEVICES set to: $NVIDIA_VISIBLE_DEVICES"
else
    echo "‚ùå NVIDIA_VISIBLE_DEVICES not set"
fi

if [ -n "$CUDA_VISIBLE_DEVICES" ]; then
    echo "‚úÖ CUDA_VISIBLE_DEVICES set to: $CUDA_VISIBLE_DEVICES"
else
    echo "‚ùå CUDA_VISIBLE_DEVICES not set"
fi

echo -e "\nüéØ Summary:"
if nvidia-smi &> /dev/null; then
    echo "‚úÖ GPU access working - Ollama should be able to use GPU"
else
    echo "‚ùå GPU access not working - Ollama will use CPU only"
    echo ""
    echo "To fix this in your pods, ensure:"
    echo "1. Container is run with --gpus all or runtime: nvidia"
    echo "2. NVIDIA Docker runtime is installed on the host"
    echo "3. NVIDIA drivers are installed on the host"
    echo "4. GPU is available and not in use by other processes"
fi
